{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c917bc",
   "metadata": {},
   "source": [
    "# Customer Churn Analysis â€” Keras Models (Jupyter Notebook)\n",
    "**Purpose:** End-to-end churn prediction project: data cleaning, EDA, visualizations, and three Keras models (including dropout and multi-feature).\n",
    "**Instructions:** Upload your dataset to the same folder and set the `DATA_PATH` variable in the first code cell. The notebook will run on a standard Python 3 environment with pandas, scikit-learn, matplotlib, seaborn, and tensorflow installed.\n",
    "Date: 27 November 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef0551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup: point DATA_PATH to your dataset (CSV or Excel) ===\n",
    "# Example: DATA_PATH = 'data/customer_churn.csv'\n",
    "DATA_PATH = 'customer_churn.csv'  # <-- change this if your file has a different name or path\n",
    "# If you uploaded an Excel file, set DATA_PATH = 'customer_churn.xlsx' and the code will handle it.\n",
    "\n",
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import os\n",
    "\n",
    "print('Python version:', sys.version if 'sys' in globals() else 'unknown') if False else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d034bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load dataset (auto-detect CSV / Excel) ===\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Upload your dataset or change DATA_PATH.\")\n",
    "\n",
    "if DATA_PATH.lower().endswith('.csv'):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "elif DATA_PATH.lower().endswith(('.xls', '.xlsx')):\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "else:\n",
    "    # Try CSV, then Excel\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_PATH)\n",
    "    except Exception as e:\n",
    "        df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "print('Dataset shape:', df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# Show column names and dtypes\n",
    "print('\\nColumn dtypes:')\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40521559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Basic cleaning ===\n",
    "# Normalize column names (optional)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Ensure TotalCharges numeric\n",
    "if 'TotalCharges' in df.columns:\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "    n_missing = df['TotalCharges'].isna().sum()\n",
    "    print('TotalCharges - coerced to numeric. Missing values:', n_missing)\n",
    "    # Fill or drop - here we fill with 0 (you can choose median or drop)\n",
    "    df['TotalCharges'].fillna(0, inplace=True)\n",
    "\n",
    "# Quick look at target column\n",
    "if 'Churn' in df.columns:\n",
    "    print('\\nChurn value counts:')\n",
    "    print(df['Churn'].value_counts())\n",
    "else:\n",
    "    print('\\nWARNING: No column named \"Churn\" found. Please ensure target column exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === A) Data Manipulation tasks ===\n",
    "# A.a - Total number of male customers\n",
    "if 'gender' in df.columns:\n",
    "    male_count = df[df['gender'].str.lower() == 'male'].shape[0]\n",
    "    print('Total male customers:', male_count)\n",
    "else:\n",
    "    print('Column \"gender\" not found.')\n",
    "\n",
    "# A.b - Total number of customers whose InternetService is DSL\n",
    "if 'InternetService' in df.columns:\n",
    "    dsl_count = df[df['InternetService'].str.lower() == 'dsl'].shape[0]\n",
    "    print('Customers with DSL InternetService:', dsl_count)\n",
    "else:\n",
    "    print('Column \"InternetService\" not found.')\n",
    "\n",
    "# A.c - Female senior citizens whose PaymentMethod is Mailed check\n",
    "new_customer_female_senior_mailed = pd.DataFrame()\n",
    "if set(['gender','SeniorCitizen','PaymentMethod']).issubset(df.columns):\n",
    "    new_customer_female_senior_mailed = df[(df['gender'].str.lower()=='female') & (df['SeniorCitizen']==1) & (df['PaymentMethod'].str.lower()=='mailed check')]\n",
    "    print('Female senior citizens with Mailed check:', new_customer_female_senior_mailed.shape[0])\n",
    "    display(new_customer_female_senior_mailed.head())\n",
    "else:\n",
    "    print('One of the columns gender/SeniorCitizen/PaymentMethod not found.')\n",
    "\n",
    "# A.d - tenure < 10 OR TotalCharges < 500\n",
    "new_customer_tenure_total = pd.DataFrame()\n",
    "if 'tenure' in df.columns and 'TotalCharges' in df.columns:\n",
    "    new_customer_tenure_total = df[(df['tenure'] < 10) | (df['TotalCharges'] < 500)]\n",
    "    print('Customers with tenure<10 OR TotalCharges<500:', new_customer_tenure_total.shape[0])\n",
    "    display(new_customer_tenure_total.head())\n",
    "else:\n",
    "    print('Columns tenure and/or TotalCharges not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === B) Visualizations ===\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# B.a - Pie chart: Churn distribution\n",
    "if 'Churn' in df.columns:\n",
    "    plt.figure(figsize=(6,6))\n",
    "    df['Churn'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Churn Distribution')\n",
    "    plt.ylabel('')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Column Churn not found - cannot plot churn distribution.')\n",
    "\n",
    "# B.b - Bar plot: InternetService distribution\n",
    "if 'InternetService' in df.columns:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(x='InternetService', data=df, order=df['InternetService'].value_counts().index)\n",
    "    plt.title('Internet Service Distribution')\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('InternetService column not found - cannot plot distribution.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602069d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Preprocessing for Modeling ===\n",
    "# Encode target\n",
    "if 'Churn' in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df['Churn_enc'] = le.fit_transform(df['Churn'])\n",
    "    print('Classes (Churn):', le.classes_)\n",
    "else:\n",
    "    raise ValueError('Target column Churn not found.')\n",
    "\n",
    "# Prepare a helper function to split and scale\n",
    "def prepare_xy(features):\n",
    "    X = df[features].copy()\n",
    "    y = df['Churn_enc'].copy()\n",
    "    # If categorical columns present, do simple encoding (one-hot)\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === C.a Model 1: tenure -> Churn ===\n",
    "features = ['tenure']\n",
    "if not set(features).issubset(df.columns):\n",
    "    raise ValueError('Required features for Model 1 not found in dataframe: ' + str(features))\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_xy(features)\n",
    "\n",
    "model1 = Sequential([\n",
    "    Dense(12, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(X_train, y_train, epochs=150, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Predictions and metrics\n",
    "y_pred1 = (model1.predict(X_test) > 0.5).astype('int32')\n",
    "print('Confusion Matrix Model 1:')\n",
    "print(confusion_matrix(y_test, y_pred1))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Epochs - Model 1\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history1.history['accuracy'], label='train_acc')\n",
    "plt.plot(history1.history.get('val_accuracy', []), label='val_acc')\n",
    "plt.title('Model 1 - Accuracy vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87025673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === C.b Model 2: with Dropout ===\n",
    "features = ['tenure']\n",
    "X_train, X_test, y_train, y_test = prepare_xy(features)\n",
    "\n",
    "model2 = Sequential([\n",
    "    Dense(12, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(X_train, y_train, epochs=150, validation_split=0.2, verbose=1)\n",
    "\n",
    "y_pred2 = (model2.predict(X_test) > 0.5).astype('int32')\n",
    "print('Confusion Matrix Model 2:')\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Epochs - Model 2\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history2.history['accuracy'], label='train_acc')\n",
    "plt.plot(history2.history.get('val_accuracy', []), label='val_acc')\n",
    "plt.title('Model 2 - Accuracy vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === C.c Model 3: tenure, MonthlyCharges, TotalCharges ===\n",
    "features = ['tenure','MonthlyCharges','TotalCharges']\n",
    "if not set(features).issubset(df.columns):\n",
    "    missing = set(features) - set(df.columns)\n",
    "    raise ValueError('Required features for Model 3 missing: ' + str(missing))\n",
    "\n",
    "X_train, X_test, y_train, y_test = prepare_xy(features)\n",
    "\n",
    "model3 = Sequential([\n",
    "    Dense(12, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history3 = model3.fit(X_train, y_train, epochs=150, validation_split=0.2, verbose=1)\n",
    "\n",
    "y_pred3 = (model3.predict(X_test) > 0.5).astype('int32')\n",
    "print('Confusion Matrix Model 3:')\n",
    "print(confusion_matrix(y_test, y_pred3))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18366a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Epochs - Model 3\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history3.history['accuracy'], label='train_acc')\n",
    "plt.plot(history3.history.get('val_accuracy', []), label='val_acc')\n",
    "plt.title('Model 3 - Accuracy vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8968bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save trained models and scaler ===\n",
    "# Creates a 'models' folder and saves models if they exist in the notebook session.\n",
    "os.makedirs('models', exist_ok=True)\n",
    "try:\n",
    "    model1.save('models/model1.h5')\n",
    "    model2.save('models/model2.h5')\n",
    "    model3.save('models/model3.h5')\n",
    "    print('Models saved to /models/*.h5')\n",
    "except Exception as e:\n",
    "    print('Could not save models:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdac97c",
   "metadata": {},
   "source": [
    "# === Closing Notes ===\n",
    "# - Change DATA_PATH to point to your dataset before running.\n",
    "# - If you want a lighter run, reduce epochs (e.g., 20) while experimenting.\n",
    "# - Consider stratified splitting if class imbalance exists.\n",
    "# - To export this notebook to PDF, use File -> Download as -> PDF (or nbconvert)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
